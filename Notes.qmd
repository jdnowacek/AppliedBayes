---
title: "MT4571: Applied Bayes Notes"
format: pdf
editor: source
---

## Chapter 1

from notes example with exponential data, gamma prior, gamma posterior
```{r}
post_mean <- 7/(6.6)
CI_left_bound <- qgamma(0.025,7,6.6)
CI_right_bound <- qgamma(0.975,7,6.6)
cat("The posterior mean is ", post_mean)
```
```{r}
cat("The 95% CI is (", CI_left_bound, ",", CI_right_bound, ")" )
```

```{r}
# Alternative approach through random sampling
sample_from_posterior <- rgamma(100000,7,6.6)
post_mean_sample <- mean(sample_from_posterior)
CI_left_bound_sample <- quantile(sample_from_posterior, 0.025)
CI_right_bound_sample <- quantile(sample_from_posterior, 0.975)
cat("The posterior mean from the random sample from the posterior is ", post_mean_sample)
```

```{r}
cat("The 95% CI from the random sample is (", CI_left_bound_sample, ",", CI_right_bound_sample, ")" )
```

# Gibbs Sampler

```{r}
#Example Gibbs sampler, for X~N(mu,sigma^2) when mu and sigma are unknown
# normal(phi,tau^2) prior on mu and IG on sigma^2(alpha,beta)
#Further down is code to run the example in OpenBUGS and JAGS

#Uncomment this line if you want to get the same answer each time you run it
set.seed(1293820)

#Priors
phi<-0
tau2<-1
alpha<-0.1
beta<-0.01

#Simulate some data
n<-10
x<-rnorm(n,0,1)
xbar<-mean(x)

#Number of samples to take in the chain
T<-100
#Create space to store the Markov chain in 
mu<-sigma2<-numeric(T)

#Set starting values
mu[1]<-4
sigma2[1]<-0.1

#Run the Gibbs sampler 
for(t in 1:(T-1)){
  mu[t+1]<-rnorm(1,(tau2*n*xbar + sigma2[t]*phi)/(tau2*n + sigma2[t]),
            sqrt(sigma2[t]*tau2/(tau2*n+sigma2[t])))
  sigma2[t+1]<-1/rgamma(1,shape=n/2 + alpha,
                          rate=1/2*sum((x-mu[t+1])^2) + beta)
}

#Produce trace plots of the outputs
par(mfrow=c(1,2))
plot(1:T,mu,xlab="Iteration",ylab="mu",type="l")
plot(1:T,sigma2,xlab="Iteration",ylab="sigma2",type="l")
par(mfrow=c(1,1))

#Produce density plots of the outputs
#(Note - you'd want to remove some initial samples as burn-in
# before using the samples for inference)
par(mfrow=c(1,2))
plot(density(mu))
plot(density(sigma2))
par(mfrow=c(1,1))

#Produce bivariate plot of samples
plot(mu,sigma2,type="p")

posteriormean_mu=mean(mu)
sortsample=sort(mu)
left95interval=(sortsample[2]+sortsample[3])/2
right95interval=(sortsample[97]+sortsample[98])/2
cat("posterior mean for mu=",posteriormean_mu, sep=c(""))
cat(" ", sep=c("\n"))
cat("95% credible interval=(",left95interval,",",right95interval,")", sep=c(""))

posteriormean_sigma2=mean(sigma2)
sortsample=sort(sigma2)
left95interval=(sortsample[2]+sortsample[3])/2
right95interval=(sortsample[97]+sortsample[98])/2
cat("posterior mean for sigma^2=",posteriormean_sigma2, sep=c(""))
cat(" ", sep=c("\n"))
cat("95% credible interval=(",left95interval,",",right95interval,")", sep=c(""))

# Of course, if you were to write your own code not as a very basic example, 
# but to properly perform some analysis,
# you would have to allow for a burn-in, and to also obtain additional inferences and 
# diagnostic plots 
```

# M-H Algorithm

```{r}
# Example of Metropolis sampling.
# Sampling from the standard Normal distribution. 

T<-500
#Variability of the normal distribution proposal
std<-sqrt(1) # Try 0.01,1,10,100 

#store samples
theta<-numeric(T)
#store number of acceptances
n.accept<-0

#starting value
theta[1]<-0
for (i in 1:(T-1)){
  phi<-rnorm(1,theta[i],std)  
  alpha<-min(c(1,exp(-0.5*(phi^2-theta[i]^2))))
  if(runif(1)<alpha) {
    theta[i+1]<-phi
    n.accept<-n.accept+1
  } else {
    theta[i+1]<-theta[i]
  }
}
p.accept<-n.accept/T

#Calculate effective sample size
library(coda)
chain<-mcmc(theta)
ess<-effectiveSize(chain)

#Plot using home-grown code
par(mfrow=c(1,2))
plot(1:T,theta,type="l",main=paste("p.accept=",format(p.accept,digits=2),", ESS=",format(ess,digits=1),sep=""))
qqnorm(theta)
#hist(theta)
par(mfrow=c(1,1))
acf(theta)
summary(theta)
sd(theta)

# For an analysis (rather than an illustration of the MH sampler properties)
#  remember to discard the burn-in sample. 
```


# Linear Regression

## Frequentist Approach

```{r}
data(mtcars)
#help(mtcars)

##creating a new dataframe with only mpg,drat,wt, and qsec as variables
vars=names(mtcars)%in%c("cyl", "disp", "hp", "vs","am","gear","carb") #variables to exclude
mtcars1=mtcars[!vars]
attach(mtcars1)

fit=lm(mpg~drat+wt+qsec,data=mtcars1)
summary(fit)
```

## Bayesian Approach

```{r}
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
library(nimble)
library(igraph)
library(coda)
library(R6)

n=nrow(mtcars1)
#covariates and response
y=mtcars1$mpg; drat=mtcars1$drat; wt=mtcars1$wt; qsec=mtcars1$qsec

# Specify the statistical model
cars_reCode <- nimbleCode({

# Specify the likelihood:
for(i in 1:n){
mu[i] <- beta1+beta2*drat[i]+beta3*wt[i]+beta4*qsec[i]
y[i]~dnorm(mu[i],tau)
}

# Prior for beta

beta1~dnorm(mu0,tau0)
beta2~dnorm(mu0,tau0)
beta3~dnorm(mu0,tau0)
beta4~dnorm(mu0,tau0)

tau0 <- 1/sigma02

# Prior for the precision
tau~dgamma(a, b)

# Compute the variance
sigma2 <- 1/tau
})
```

```{r}
# hyperparameters for the betas and tau
# would find n in R environment. Including here to suppress a warning
cars_reConsts <- list(mu0=0, sigma02=1000, a=0.1, b=0.1, n=32)

# list with data
cars_reData <- list(y=y,drat=drat,wt=wt,qsec=qsec)

# Initial values
cars_reInits <- list(beta1=0.5, tau = 1) # Nimble will generate the rest 

# Build and Compile the model
# to build the model
cars_re <- nimbleModel(code = cars_reCode, name = "cars_re", constants = cars_reConsts,
                    data = cars_reData, inits<-cars_reInits)

# To compile the model
Ccars_re <- compileNimble(cars_re)

# set up the monitored quantities. Default is all of the random quantities
cars_reConf <- configureMCMC(cars_re, monitors = c('beta1', 'beta2', 
              'beta3', 'beta4', 'sigma2'), enableWAIC = TRUE, print = TRUE) 

# build the MCMC algorithm
cars_reMCMC <- buildMCMC(cars_reConf)

# compile the MCMC chain 
Ccars_reMCMC <- compileNimble(cars_reMCMC, project = cars_re)
```

```{r}
####################################################################################
####### POSTERIOR SAMPLES IN CODA FORMAT TO GET MORE EASILY PLOTS AND DIAGNOSTICS  #
####################################################################################
set.seed(10)
cars_reInits <- list(list(beta1 = 0, tau = 1), 
                     list(beta1 = 1, tau = 2))
posterior <- runMCMC(Ccars_reMCMC, niter = 500000, thin=50, nburnin=100000, 
                     summary = TRUE, WAIC = TRUE, samples = TRUE, nchains=2, 
                     samplesAsCodaMCMC=TRUE, inits = cars_reInits) 
```

```{r}
combinedchains <- mcmc.list(posterior$samples$chain1, posterior$samples$chain2)
plot(combinedchains) # too many plots sometimes
```

```{r}
#plot(combinedchains[,c('beta1','beta2','sigma2')]) 
#plot(combinedchains[,'beta[1]']) # when defining a vector of parameters
autocorr.plot(posterior$samples$chain1)

autocorr.plot(posterior$samples$chain2)
```


```{r}
gelman.diag(combinedchains)

gelman.plot(combinedchains)
```

```{r}
summary(combinedchains)
```

```{r}
ESS <- t(effectiveSize(combinedchains))
cat("The Effective Sample Size is ", ESS)
```









