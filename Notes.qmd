---
title: "MT4571: Applied Bayes Notes"
format: pdf
editor: source
---

## Chapter 1

from notes example with exponential data, gamma prior, gamma posterior
```{r}
post_mean <- 7/(6.6)
CI_left_bound <- qgamma(0.025,7,6.6)
CI_right_bound <- qgamma(0.975,7,6.6)
cat("The posterior mean is ", post_mean)
```
```{r}
cat("The 95% CI is (", CI_left_bound, ",", CI_right_bound, ")" )
```

```{r}
# Alternative approach through random sampling
sample_from_posterior <- rgamma(100000,7,6.6)
post_mean_sample <- mean(sample_from_posterior)
CI_left_bound_sample <- quantile(sample_from_posterior, 0.025)
CI_right_bound_sample <- quantile(sample_from_posterior, 0.975)
cat("The posterior mean from the random sample from the posterior is ", post_mean_sample)
```

```{r}
cat("The 95% CI from the random sample is (", CI_left_bound_sample, ",", CI_right_bound_sample, ")" )
```

# Gibbs Sampler

```{r}
#Example Gibbs sampler, for X~N(mu,sigma^2) when mu and sigma are unknown
# normal(phi,tau^2) prior on mu and IG on sigma^2(alpha,beta)
#Further down is code to run the example in OpenBUGS and JAGS

#Uncomment this line if you want to get the same answer each time you run it
set.seed(1293820)

#Priors
phi<-0
tau2<-1
alpha<-0.1
beta<-0.01

#Simulate some data
n<-10
x<-rnorm(n,0,1)
xbar<-mean(x)

#Number of samples to take in the chain
T<-100
#Create space to store the Markov chain in 
mu<-sigma2<-numeric(T)

#Set starting values
mu[1]<-4
sigma2[1]<-0.1

#Run the Gibbs sampler 
for(t in 1:(T-1)){
  mu[t+1]<-rnorm(1,(tau2*n*xbar + sigma2[t]*phi)/(tau2*n + sigma2[t]),
            sqrt(sigma2[t]*tau2/(tau2*n+sigma2[t])))
  sigma2[t+1]<-1/rgamma(1,shape=n/2 + alpha,
                          rate=1/2*sum((x-mu[t+1])^2) + beta)
}

#Produce trace plots of the outputs
par(mfrow=c(1,2))
plot(1:T,mu,xlab="Iteration",ylab="mu",type="l")
plot(1:T,sigma2,xlab="Iteration",ylab="sigma2",type="l")
par(mfrow=c(1,1))

#Produce density plots of the outputs
#(Note - you'd want to remove some initial samples as burn-in
# before using the samples for inference)
par(mfrow=c(1,2))
plot(density(mu))
plot(density(sigma2))
par(mfrow=c(1,1))

#Produce bivariate plot of samples
plot(mu,sigma2,type="p")

posteriormean_mu=mean(mu)
sortsample=sort(mu)
left95interval=(sortsample[2]+sortsample[3])/2
right95interval=(sortsample[97]+sortsample[98])/2
cat("posterior mean for mu=",posteriormean_mu, sep=c(""))
cat(" ", sep=c("\n"))
cat("95% credible interval=(",left95interval,",",right95interval,")", sep=c(""))

posteriormean_sigma2=mean(sigma2)
sortsample=sort(sigma2)
left95interval=(sortsample[2]+sortsample[3])/2
right95interval=(sortsample[97]+sortsample[98])/2
cat("posterior mean for sigma^2=",posteriormean_sigma2, sep=c(""))
cat(" ", sep=c("\n"))
cat("95% credible interval=(",left95interval,",",right95interval,")", sep=c(""))

# Of course, if you were to write your own code not as a very basic example, 
# but to properly perform some analysis,
# you would have to allow for a burn-in, and to also obtain additional inferences and 
# diagnostic plots 
```

# M-H Algorithm

```{r}
# Example of Metropolis sampling.
# Sampling from the standard Normal distribution. 

T<-500
#Variability of the normal distribution proposal
std<-sqrt(1) # Try 0.01,1,10,100 

#store samples
theta<-numeric(T)
#store number of acceptances
n.accept<-0

#starting value
theta[1]<-0
for (i in 1:(T-1)){
  phi<-rnorm(1,theta[i],std)  
  alpha<-min(c(1,exp(-0.5*(phi^2-theta[i]^2))))
  if(runif(1)<alpha) {
    theta[i+1]<-phi
    n.accept<-n.accept+1
  } else {
    theta[i+1]<-theta[i]
  }
}
p.accept<-n.accept/T

#Calculate effective sample size
library(coda)
chain<-mcmc(theta)
ess<-effectiveSize(chain)

#Plot using home-grown code
par(mfrow=c(1,2))
plot(1:T,theta,type="l",main=paste("p.accept=",format(p.accept,digits=2),", ESS=",format(ess,digits=1),sep=""))
qqnorm(theta)
#hist(theta)
par(mfrow=c(1,1))
acf(theta)
summary(theta)
sd(theta)

# For an analysis (rather than an illustration of the MH sampler properties)
#  remember to discard the burn-in sample. 
```









